{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa202020/CCDMR/blob/main/CCDMR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXlDE81XhILq"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "!pip install sktime\n",
        "!pip install xgboost\n",
        "!pip install javalang\n",
        "!pip install openpyxl xlwt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0S8tLbLISPj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp  /content/drive/MyDrive/bcb.rar  /content/\n",
        "!apt-get install -y unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjia1Kw3KaEL"
      },
      "outputs": [],
      "source": [
        "!cp  /content/drive/MyDrive/clonedatasetfinal/merged_with_T1.csv  /content/\n",
        "!cp  /content/drive/MyDrive/clonedatasetfinal/merged_with_T4.csv  /content/\n",
        "!cp  /content/drive/MyDrive/clonedatasetfinal/merged_with_ST3.csv  /content/\n",
        "!cp  /content/drive/MyDrive/clonedatasetfinal/merged_with_T2.csv  /content/\n",
        "!cp  /content/drive/MyDrive/clonedatasetfinal/merged_with_MT3.csv  /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg9Wj63wgB8a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX7b3RkQIm_w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace 'your_file.rar' with the name of your uploaded RAR file\n",
        "rar_file_path ='/content/bcb.rar'# '/content/id2sourcecode.rar'\n",
        "extraction_path = '/content/java_files/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Extract the RAR file\n",
        "!unrar x -y {rar_file_path} {extraction_path}\n",
        "\n",
        "print(f\"Files extracted to {extraction_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOiSV3jJIj6Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WFX0louf74l",
        "outputId": "d5943d70-7e68-4682-9e63-b0d18920b2d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of files in directory '/content/java_files': 0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "def count_files_in_directory(directory_path):\n",
        "    total_files = 0\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        total_files += len(files)\n",
        "    return total_files\n",
        "\n",
        "# Directory to count files in\n",
        "directory_path = '/content/java_files'\n",
        "\n",
        "\n",
        "# Count files\n",
        "num_files = count_files_in_directory(directory_path)\n",
        "print(f\"Total number of files in directory '{directory_path}': {num_files}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd1FfzFOmawA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ_AKGxwKjIy"
      },
      "outputs": [],
      "source": [
        "#java's to sequences\n",
        "import os\n",
        "\n",
        "import glob\n",
        "import javalang\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import javalang\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "token_dict = {}\n",
        "token_dictt = {}\n",
        "def tokenize_code(code):\n",
        "    global token_dict\n",
        "    global token_dictt\n",
        "\n",
        "    try:\n",
        "        tokens = list(javalang.tokenizer.tokenize(code))\n",
        "    except javalang.tokenizer.LexerError as e:\n",
        "        print(f\"LexerError: {e}\")\n",
        "        print(\"The problematic code segment is likely to have an unterminated character/string literal or other lexical error.\")\n",
        "        return []\n",
        "\n",
        "    token_sequence = []\n",
        "\n",
        "    for token in tokens:\n",
        "        token_value = token.value\n",
        "        token_type = str(type(token))[:-2].split(\".\")[-1]\n",
        "\n",
        "        if isinstance(token, javalang.tokenizer.Identifier):\n",
        "            token_value = 'id'\n",
        "        if isinstance(token, javalang.tokenizer.String):\n",
        "            token_value = 'string'\n",
        "\n",
        "        if token_value not in token_dict:\n",
        "            token_dict[token_value] = len(token_dict) + 1  # Start token numbering from 1\n",
        "\n",
        "        if token_type not in token_dictt:\n",
        "            token_dictt[token_type] = len(token_dictt) + 1\n",
        "\n",
        "        token_sequence.append(token_dict[token_value])\n",
        "        token_sequence.append(token_dictt[token_type])\n",
        "\n",
        "\n",
        "    return token_sequence\n",
        "import re\n",
        "import re\n",
        "\n",
        "def remove_comments(source_code):\n",
        "\n",
        "    # Remove block comments\n",
        "    source_code = re.sub(r'/\\*.*?\\*/', '', source_code, flags=re.DOTALL)\n",
        "    # Remove line comments\n",
        "    source_code = re.sub(r'//.*', '', source_code)\n",
        "    return source_code\n",
        "\n",
        "def clean_source_code(source_code):\n",
        "\n",
        "    return source_code.encode('ascii', 'ignore').decode('ascii')\n",
        "def getCodeBlock(file_path):\n",
        "    global token_dict\n",
        "    global token_dictt\n",
        "\n",
        "    block = []\n",
        "    # print(file_path)\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as temp_file:\n",
        "            lines = temp_file.readlines()\n",
        "            for line in lines:\n",
        "                try:\n",
        "                    tokens = list(javalang.tokenizer.tokenize(line))\n",
        "                    for token in tokens:\n",
        "                        token_value = token.value\n",
        "                        token_type = str(type(token))[:-2].split(\".\")[-1]\n",
        "\n",
        "                        if isinstance(token, javalang.tokenizer.Identifier):\n",
        "                                               token_value = 'id'\n",
        "                        if isinstance(token, javalang.tokenizer.String):\n",
        "                                               token_value = 'string'\n",
        "\n",
        "                        if token_value not in token_dict:\n",
        "                                 token_dict[token_value] = len(token_dict) + 1  # Start token numbering from 1\n",
        "\n",
        "                        if token_type not in token_dictt:\n",
        "                                  token_dictt[token_type] = len(token_dictt) + 1\n",
        "\n",
        "                        #block.append(token_dict[token_value])\n",
        "                        block.append(token_dictt[token_type])\n",
        "\n",
        "\n",
        "                except javalang.tokenizer.LexerError as e:\n",
        "                    print(f\"LexerError in file {file_path} at line: {line.strip()}: {e}\")\n",
        "                    # Optionally handle the line with the error, e.g., skip it\n",
        "                    continue\n",
        "    except UnicodeDecodeError as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        return None\n",
        "    return block\n",
        "\n",
        "def process_java_files(input_dir, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    java_files = glob.glob(os.path.join(input_dir, '*.java'))\n",
        "      # Initialize the dictionary outside the loop\n",
        "\n",
        "    for java_file in java_files:\n",
        "\n",
        "        token_sequence = getCodeBlock(java_file)\n",
        "        token_sequence_str = ' '.join(map(str, token_sequence))\n",
        "\n",
        "        base_filename = os.path.basename(java_file)\n",
        "        output_filename = os.path.splitext(base_filename)[0] + '.txt'\n",
        "        output_file_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(token_sequence_str)\n",
        "            #print(token_sequence_str)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_directory = '/content/java_files'  # Replace with the path to the directory containing Java files\n",
        "output_directory = '/content/sequencessall'  # Replace with the path to the directory where you want to save the output\n",
        "  # Replace with the path to the directory where you want to save the output\n",
        "\n",
        "process_java_files(input_directory, output_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SB8XzjGDfn7X"
      },
      "outputs": [],
      "source": [
        "#fianl version with allclassifiers here for test 3dnumpy and multii\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, classification_report, f1_score\n",
        "from sktime.transformations.panel.rocket import Rocket,MiniRocket\n",
        "from sktime.datatypes._panel._convert import from_2d_array_to_nested\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#make a change of parameters\n",
        "clone='MT3'\n",
        "cl='MT3'\n",
        "pathcsv ='/content/drive/MyDrive/clonedatasetfinal/merged_with_'+str(cl)+'.csv'#'/content/MT3_allinone.csv'# '/content/drive/MyDrive/id2sourcecode/sequencesall'\n",
        "#pathcsv = 'D:/BCB/astn/clone_4.csv'\n",
        "time_series_dir = '/content/sequencessall'\n",
        "rl=1#step\n",
        "num_kernels_list = [500,1000,10000]\n",
        "def count_numbers_in_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "    numbers = content.split()\n",
        "    number_count = len(numbers)\n",
        "    return number_count\n",
        "\n",
        "# Function to get the length of a time series\n",
        "def get_time_series_length(file):\n",
        "    if not os.path.exists(file):\n",
        "        return 0\n",
        "    ts = np.loadtxt(file)\n",
        "    return len(ts)\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "def load_and_merge_time_series(file1, file2, max_length):\n",
        "    if not os.path.exists(file1):\n",
        "        print(f\"File not found: {file1}\")\n",
        "        return None\n",
        "    if not os.path.exists(file2):\n",
        "        print(f\"File not found: {file2}\")\n",
        "        return None\n",
        "\n",
        "    ts1 = np.loadtxt(file1)\n",
        "    ts2 = np.loadtxt(file2)\n",
        "    concatenated_ts = np.concatenate((ts1, ts2))\n",
        "    concatenated_ts  = np.pad( concatenated_ts , (0, 2*max_length - len( concatenated_ts )), mode='constant')\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ts1 = np.pad(ts1, (0, max_length - len(ts1)), mode='constant')\n",
        "    ts2 = np.pad(ts2, (0, max_length - len(ts2)), mode='constant')\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #differential_encoded_ts = np.diff(concatenated_ts, prepend=0)\n",
        "    return  concatenated_ts\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import time\n",
        "\n",
        "# Function to evaluate classifiers\n",
        "def evaluate_classifiers(X_train, X_test, y_train, y_test, classifiers):\n",
        "    results = {}\n",
        "    for name, clf in classifiers.items():\n",
        "        start_time = time.time()\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\n",
        "        y_scores = clf.predict_proba(X_test)[:, 1] if hasattr(clf, \"predict_proba\") else clf.decision_function(X_test)\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        end_time = time.time()\n",
        "        results[name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'report': report,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'fpr': fpr,\n",
        "            'tpr': tpr,\n",
        "            'roc_auc': roc_auc,\n",
        "            'time': end_time - start_time\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Path to CSV file and directory containing time series files\n",
        "\n",
        "\n",
        "\n",
        "# Load CSV with file names and class labels\n",
        "df_labels = pd.read_csv(pathcsv)\n",
        "\n",
        "# Determine the maximum length of time series\n",
        "\n",
        "max_length = 0\n",
        "\n",
        "min_length=0\n",
        "av=0\n",
        "l=0\n",
        "for i in range(0, len(df_labels), rl):\n",
        "    row = df_labels.iloc[i]\n",
        "    \"\"\"\n",
        "    file1 = os.path.join(time_series_dir, str(row['id1']).strip() + '.txt')\n",
        "    file2 = os.path.join(time_series_dir, str(row['id2']).strip() + '.txt')\n",
        "    \"\"\"\n",
        "    file1 = os.path.join(time_series_dir, str(row['FunID1']).strip() + '.txt')\n",
        "    file2 = os.path.join(time_series_dir, str(row['FunID2']).strip() + '.txt')\n",
        "\n",
        "    ts1_length = get_time_series_length(file1)\n",
        "    ts2_length = get_time_series_length(file2)\n",
        "\n",
        "    max_length = max(max_length, ts1_length, ts2_length)\n",
        "    min_length = min(min_length, ts1_length, ts2_length)\n",
        "    #av=(av+(ts1_length+ts1_length)/2)/2\n",
        "    l=l+2\n",
        "max_length=(max_length)\n",
        "print(\"max length nhnfgfor \",max_length, \"min=\",min, \"av\",av/l)\n",
        "#max_length=count+3#(count//2)+3\n",
        "\n",
        "\n",
        "# Extract features and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for i in range(0, len(df_labels), rl):\n",
        "    row = df_labels.iloc[i]\n",
        "    \"\"\"\n",
        "    file1 = os.path.join(time_series_dir, str(row['id1']).strip() + '.txt')\n",
        "    file2 = os.path.join(time_series_dir, str(row['id2']).strip() + '.txt')\n",
        "    label = row['label']\n",
        "    \"\"\"\n",
        "    file1 = os.path.join(time_series_dir, str(row['FunID1']).strip() + '.txt')\n",
        "    file2 = os.path.join(time_series_dir, str(row['FunID2']).strip() + '.txt')\n",
        "    label = row['Type']\n",
        "\n",
        "    merged_ts = load_and_merge_time_series(file1, file2, max_length)\n",
        "\n",
        "    #print(\"mergerd is herer\",merged_ts)\n",
        "    if merged_ts is not None:\n",
        "        features.append(merged_ts)\n",
        "        labels.append(label)\n",
        "        #print(\"rrrrererer\")\n",
        "\n",
        "if not features:\n",
        "    raise ValueError(\"No valid time series data found.\")\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "time_series_array = np.array(features)\n",
        "\n",
        "# Create a MultiIndex with the series index and time step index\n",
        "index = pd.MultiIndex.from_product(\n",
        "    [range(time_series_array.shape[0]), range(time_series_array.shape[1])],\n",
        "    names=['series', 'time_step']\n",
        ")\n",
        "\n",
        "# Flatten the array to fit into the DataFrame\n",
        "flattened_array = time_series_array.flatten()\n",
        "\n",
        "# Create the DataFrame with the MultiIndex\n",
        "df_multiindex = pd.DataFrame(flattened_array, index=index, columns=[\"value\"])\n",
        "\n",
        "print(df_multiindex)\n",
        "\n",
        "# Plot the third time series (index 2 since Python uses 0-based indexing)\n",
        "#df_multiindex['series'].iloc[2].plot()\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Time Series ')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "print(labels)\n",
        "\n",
        "from sktime.datatypes import check_raise\n",
        "#from sktime.datatypes import check_is_pd-multiindex\n",
        "\n",
        "\n",
        "\n",
        "features=df_multiindex\n",
        "print(\"checking if features is conform to multiindex\")\n",
        "print(check_raise(features, mtype=\"pd-multiindex\"))\n",
        "#check_is_pd-multiindex(features, mtype=\"pd-multiindex\", return_metadata=True)\n",
        "labels = np.array(labels)\n",
        "labels = np.where(labels == cl, 1, 0)\n",
        "print(labels)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming labels is your array and cl is the condition\n",
        "\n",
        "\n",
        "# Count the number of 1s and 0s\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "count_dict = dict(zip(unique, counts))\n",
        "for class_label, count in count_dict.items():\n",
        "    print(f\"Class {class_label}: {count} instances\")\n",
        "\n",
        "# Create a bar chart\n",
        "plt.bar(count_dict.keys(), count_dict.values(), color=['blue', 'orange'])\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of 1s and 0s in labels')\n",
        "plt.xticks([0, 1], ['0', '1'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "unique=0\n",
        "counts =0\n",
        "features_df=0\n",
        "print(len(features))\n",
        "# List of classifiers\n",
        "\"\"\"\n",
        "Logistic Regression': LogisticRegression(max_iter=100),\n",
        "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'SVM': SVC()}\n",
        "\"\"\"\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=100),\n",
        "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Vary number of kernels and evaluate\n",
        "#,1150,1300]#[600, 500,700 ,900]\n",
        "all_results = {}\n",
        "print(\"training\")\n",
        "import joblib\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "# Assuming trf is your MiniRocket instance\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "for num_kernels in num_kernels_list:\n",
        "    print(f\"\\nTraining MiniRocket with {num_kernels} kernels...\")\n",
        "    trf_start_time = time.time()\n",
        "    trf = MiniRocket(num_kernels=num_kernels)\n",
        "    trf.fit(features)\n",
        "    trf_end_time = time.time()\n",
        "    trf_training_time = trf_end_time - trf_start_time\n",
        "    joblib.dump(trf, f'/content/drive/MyDrive/mT3ker600mini_rocket_model{num_kernels}T4.pkl')\n",
        "    trftr_start_time = time.time()\n",
        "    transformed_features = trf.transform(features)\n",
        "    trftr_end_time = time.time()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(transformed_features, labels, test_size=0.5, random_state=42, shuffle=True)\n",
        "\n",
        "    print(\"shape xtr\",X_train.shape)\n",
        "    lrow =X_train[1]\n",
        "    print(len(lrow), lrow)\n",
        "\n",
        "    print(\"wtrain\", len(X_train),len (X_train[0]),\"=\",len(y_train))\n",
        "    print(\"wtest\", len(X_test),len(X_test[0]) ,\"=\",len(y_train))\n",
        "    print(y_train)\n",
        "    print(y_test)\n",
        "\n",
        "\n",
        "\n",
        "    results = {}\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"\\nEvaluating {name}...\")\n",
        "        clf_start_time = time.time()\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        clf_end_time = time.time()\n",
        "\n",
        "        if hasattr(clf, \"decision_function\"):\n",
        "            y_scores = clf.decision_function(X_test)\n",
        "        else:\n",
        "            y_scores = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        results[name] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score(y_test, y_pred),\n",
        "            'roc_auc': roc_auc,\n",
        "            'inference_time': clf_end_time - clf_start_time,\n",
        "            'training_time': trf_training_time,\n",
        "            'transformation':trftr_end_time - trftr_start_time\n",
        "        }\n",
        "\n",
        "        # Print textual results\n",
        "        print(f\"Classifier: {name}\")\n",
        "        print(f\"Precision: {precision.mean()}\")\n",
        "        print(f\"Recall: {recall.mean()}\")\n",
        "        print(f\"F1 Score: {results[name]['f1_score']}\")\n",
        "        print(f\"ROC AUC: {results[name]['roc_auc']}\")\n",
        "        print(f\"Inference Time: {results[name]['inference_time']} seconds\")\n",
        "        print(f\"MiniRocket Training Time: {results[name]['training_time']} seconds\")\n",
        "\n",
        "    all_results[num_kernels] = results\n",
        "    print('tran', trftr_end_time - trftr_start_time)\n",
        "\n",
        "results_list = []\n",
        "for num_kernels, results in all_results.items():\n",
        "    for name, metrics in results.items():\n",
        "        results_list.append({\n",
        "            'Num Kernels': num_kernels,\n",
        "            'Classifier': name,\n",
        "            'Precision': metrics['precision'],\n",
        "            'Recall': metrics['recall'],\n",
        "            'F1 Score': metrics['f1_score'],\n",
        "            'ROC AUC': metrics['roc_auc'],\n",
        "            'Inference Time (s)': metrics['inference_time'],\n",
        "            'MiniRocket Training Time (s)': metrics['training_time'],\n",
        "            'MiniRocket Trnsformation Time (s)': metrics['transformation']\n",
        "        })\n",
        "\n",
        "df_results = pd.DataFrame(results_list)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df_results)\n",
        "pth ='/content/drive/MyDrive/resultpaddafter/finalnewresults_'+str(cl)+'.csv'\n",
        "df_results.to_csv(pth, index=False)\n",
        "pthxls ='/content/drive/MyDrive/resultpaddafter/finalnewresults_'+str(cl)+'.xls'\n",
        "df_results.to_excel(pthxls, index=False,engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVXYi3xO6pU1"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tbb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1FxX4AYMPaxY0TbrG3N-w6EljJTP1l5ty",
      "authorship_tag": "ABX9TyPmOt9Mvkxg25BWq8e4u63H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}